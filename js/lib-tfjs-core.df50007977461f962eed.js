/*! For license information please see lib-tfjs-core.df50007977461f962eed.js.LICENSE.txt */
"use strict";(self.webpackChunktheremin=self.webpackChunktheremin||[]).push([[928],{2692:(t,a,e)=>{e.d(a,{CQI:()=>s.l}),e(5389),e(3807),e(6282);var n=e(7760),r=e(198),s=e(3687),i=e(748),o=e(1462),c=e(5527),l=(e(5280),e(5492),e(3077));e(8497),e(3801),e(9166),e(586);o.b,c.b,n.f,n.d,n.k,n.g,n.e,n.c,n.a,c.h,c.i,c.l,n.r,n.l,c.w,n.m,n.n,n.o,n.p,(0,r.o)({confusionMatrix_:function(t,a,e){const n=(0,r.q)(t,"labels","confusionMatrix"),s=(0,r.q)(a,"predictions","confusionMatrix");(0,l.a)(null==e||e>0&&Number.isInteger(e),(()=>`If provided, numClasses must be a positive integer, but got ${e}`)),(0,l.a)(1===n.rank,(()=>`Expected the rank of labels to be 1, but got ${n.rank}`)),(0,l.a)(1===s.rank,(()=>`Expected the rank of predictions to be 1, but got ${s.rank}`)),(0,l.a)(n.shape[0]===s.shape[0],(()=>`Mismatch in the number of examples: ${n.shape[0]} vs. ${s.shape[0]}. Labels and predictions should have the same number of elements.`)),(0,l.a)(e>0&&Number.isInteger(e),(()=>`numClasses is required to be a positive integer, but got ${e}`));const o=(0,i.ak)((0,r.c)(n,"int32"),e),c=(0,i.ak)((0,r.c)(s,"int32"),e),u=(0,i.aD)(o),m=(0,i.aF)(u,c);return(0,r.c)(m,"int32")}});class u{constructor(){this.classNameMap={}}static getMap(){return null==u.instance&&(u.instance=new u),u.instance}static register(t){u.getMap().classNameMap[t.className]=[t,t.fromConfig]}}function m(t){(0,l.a)(null!=t.className,(()=>"Class being registered does not have the static className property defined.")),(0,l.a)("string"==typeof t.className,(()=>"className is required to be a string, but got type "+typeof t.className)),(0,l.a)(t.className.length>0,(()=>"Class being registered has an empty-string as its className, which is disallowed.")),u.register(t)}(0,r.o)({basicLSTMCell_:function(t,a,e,n,s,o){const c=(0,r.q)(t,"forgetBias","basicLSTMCell"),l=(0,r.q)(a,"lstmKernel","basicLSTMCell"),u=(0,r.q)(e,"lstmBias","basicLSTMCell"),m=(0,r.q)(n,"data","basicLSTMCell"),h=(0,r.q)(s,"c","basicLSTMCell"),d=(0,r.q)(o,"h","basicLSTMCell"),b=(0,i.h)([m,d],1),p=(0,i.aF)(b,l),g=(0,i.b)(p,u),f=g.shape[0],k=g.shape[1]/4,v=[f,k],y=(0,i.c)(g,[0,0],v),N=(0,i.c)(g,[0,k],v),E=(0,i.c)(g,[0,2*k],v),M=(0,i.c)(g,[0,3*k],v),$=(0,i.b)((0,r.m)((0,r.s)(y),(0,i.y)(N)),(0,r.m)(h,(0,r.s)((0,i.b)(c,E))));return[$,(0,r.m)((0,i.y)($),(0,r.s)(M))]}}),(0,r.o)({batchNorm2d_:function(t,a,e,n,s,o){const c=(0,r.q)(t,"x","batchNorm"),u=(0,r.q)(a,"mean","batchNorm"),m=(0,r.q)(e,"variance","batchNorm");let h,d;return null!=s&&(h=(0,r.q)(s,"scale","batchNorm")),null!=n&&(d=(0,r.q)(n,"offset","batchNorm")),(0,l.a)(2===c.rank,(()=>`Error in batchNorm2D: x must be rank 2 but got rank ${c.rank}.`)),(0,l.a)(2===u.rank||1===u.rank,(()=>`Error in batchNorm2D: mean must be rank 2 or rank 1 but got rank ${u.rank}.`)),(0,l.a)(2===m.rank||1===m.rank,(()=>`Error in batchNorm2D: variance must be rank 2 or rank 1 but got rank ${m.rank}.`)),null!=h&&(0,l.a)(2===h.rank||1===h.rank,(()=>`Error in batchNorm2D: scale must be rank 2 or rank 1 but got rank ${h.rank}.`)),null!=d&&(0,l.a)(2===d.rank||1===d.rank,(()=>`Error in batchNorm2D: offset must be rank 2 or rank 1 but got rank ${d.rank}.`)),(0,i.aK)(c,u,m,d,h,o)}}),(0,r.o)({batchNorm3d_:function(t,a,e,n,s,o){const c=(0,r.q)(t,"x","batchNorm"),u=(0,r.q)(a,"mean","batchNorm"),m=(0,r.q)(e,"variance","batchNorm");let h,d;return null!=s&&(h=(0,r.q)(s,"scale","batchNorm")),null!=n&&(d=(0,r.q)(n,"offset","batchNorm")),(0,l.a)(3===c.rank,(()=>`Error in batchNorm3D: x must be rank 3 but got rank ${c.rank}.`)),(0,l.a)(3===u.rank||1===u.rank,(()=>`Error in batchNorm3D: mean must be rank 3 or rank 1 but got rank ${u.rank}.`)),(0,l.a)(3===m.rank||1===m.rank,(()=>`Error in batchNorm3D: variance must be rank 3 or rank 1 but got rank ${m.rank}.`)),null!=h&&(0,l.a)(3===h.rank||1===h.rank,(()=>`Error in batchNorm3D: scale must be rank 3 or rank 1 but got rank ${h.rank}.`)),null!=d&&(0,l.a)(3===d.rank||1===d.rank,(()=>`Error in batchNorm3D: offset must be rank 3 or rank 1 but got rank ${d.rank}.`)),(0,i.aK)(c,u,m,d,h,o)}}),(0,r.o)({batchNorm4d_:function(t,a,e,n,s,o){const c=(0,r.q)(t,"x","batchNorm"),u=(0,r.q)(a,"mean","batchNorm"),m=(0,r.q)(e,"variance","batchNorm");let h,d;return null!=s&&(h=(0,r.q)(s,"scale","batchNorm")),null!=n&&(d=(0,r.q)(n,"offset","batchNorm")),(0,l.a)(4===c.rank,(()=>`Error in batchNorm4D: x must be rank 4 but got rank ${c.rank}.`)),(0,l.a)(4===u.rank||1===u.rank,(()=>`Error in batchNorm4D: mean must be rank 4 or rank 1 but got rank ${u.rank}.`)),(0,l.a)(4===m.rank||1===m.rank,(()=>`Error in batchNorm4D: variance must be rank 4 or rank 1 but got rank ${m.rank}.`)),null!=h&&(0,l.a)(4===h.rank||1===h.rank,(()=>`Error in batchNorm4D: scale must be rank 4 or rank 1 but got rank ${h.rank}.`)),null!=d&&(0,l.a)(4===d.rank||1===d.rank,(()=>`Error in batchNorm4D: offset must be rank 4 or rank 1 but got rank ${d.rank}.`)),(0,i.aK)(c,u,m,d,h,o)}}),(0,r.o)({concat1d_:function(t){return(0,i.h)(t,0)}}),(0,r.o)({concat2d_:function(t,a){return(0,i.h)(t,a)}}),(0,r.o)({concat3d_:function(t,a){return(0,i.h)(t,a)}}),(0,r.o)({concat4d_:function(t,a){return(0,i.h)(t,a)}});const h=(0,r.o)({conv3DBackpropInput_:function(t,a,e,n,s){(0,l.a)(t.length===a.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${a.rank}) must match`));let i=t,o=a,c=!1;4===a.rank&&(c=!0,o=(0,r.r)(a,[1,a.shape[0],a.shape[1],a.shape[2],a.shape[3]]),i=[1,t[0],t[1],t[2],t[3]]);const u=i[4],m=o.shape[4];(0,l.a)(5===i.length,(()=>`Error in conv3dDerInput: inShape must be length 5, but got length ${i.length}.`)),(0,l.a)(5===o.rank,(()=>`Error in conv3dDerInput: dy must be rank 5, but got rank ${o.rank}`)),(0,l.a)(5===e.rank,(()=>`Error in conv3dDerInput: filter must be rank 5, but got rank ${e.rank}`)),(0,l.a)(u===e.shape[3],(()=>`Error in conv3dDerInput: depth of input (${u}) must match input depth for filter ${e.shape[3]}.`)),(0,l.a)(m===e.shape[4],(()=>`Error in conv3dDerInput: depth of output (${m}) must match output depth for filter ${e.shape[4]}.`));const h={dy:o,filter:e},d={pad:s,strides:n,inputShape:i},b=r.E.runKernel(r.cw,h,d);return c?(0,r.r)(b,[b.shape[1],b.shape[2],b.shape[3],b.shape[4]]):b}});(0,r.o)({conv3dTranspose_:function(t,a,e,n,s){const i=(0,r.q)(t,"x","conv3dTranspose"),o=(0,r.q)(a,"filter","conv3dTranspose");return h(e,i,o,n,s)}}),(0,r.o)({diag_:function(t){const a={x:(0,r.q)(t,"x","diag")};return r.E.runKernel(r.cx,a)}}),(0,r.o)({dot_:function(t,a){const e=(0,r.q)(t,"t1","dot"),n=(0,r.q)(a,"t2","dot");(0,l.a)(!(1!==e.rank&&2!==e.rank||1!==n.rank&&2!==n.rank),(()=>`Error in dot: inputs must all be rank 1 or 2, but got ranks ${e.rank} and ${n.rank}.`));const s=1===e.rank?e.size:e.shape[1],o=1===n.rank?n.size:n.shape[0];if((0,l.a)(s===o,(()=>`Error in dot: inner dimensions of inputs must match, but got ${s} and ${o}.`)),1===e.rank&&1===n.rank){const t=(0,r.r)(e,[1,-1]),a=(0,r.r)(n,[-1,1]),s=(0,i.aF)(t,a);return(0,r.r)(s,[])}if(1===e.rank&&2===n.rank){const t=(0,r.r)(e,[1,-1]),a=(0,r.r)(n,[n.shape[0],n.shape[1]]),s=(0,i.aF)(t,a);return(0,r.r)(s,[s.size])}if(2===e.rank&&1===n.rank){const t=(0,r.r)(n,[-1,1]),a=(0,i.aF)(e,t);return(0,r.r)(a,[a.size])}{const t=(0,r.r)(n,[n.shape[0],n.shape[1]]);return(0,i.aF)(e,t)}}}),(0,r.o)({isFinite_:function(t){const a={x:(0,r.q)(t,"x","isFinite")};return r.E.runKernel(r.cA,a)}}),(0,r.o)({isInf_:function(t){const a={x:(0,r.q)(t,"x","isInf")};return r.E.runKernel(r.cB,a)}}),(0,r.o)({logSigmoid_:function(t){const a=(0,r.q)(t,"x","logSigmoid"),e=(0,i.bd)((t=>({value:(0,i.J)((0,i.B)((0,i.J)(t))),gradFunc:a=>(0,r.m)(a,(0,r.s)((0,i.J)(t)))})));return e(a)}}),(0,r.o)({logicalXor_:function(t,a){const e=(0,r.q)(t,"a","logicalXor","bool"),n=(0,r.q)(a,"b","logicalXor","bool");return(0,r.ae)(e.shape,n.shape),(0,i.av)((0,i.at)(t,a),(0,i.au)((0,i.av)(t,a)))}}),(0,r.o)({moments_:function(t,a=null,e=!1){t=(0,r.q)(t,"x","moments");const n=(0,l.p)(a,t.shape),s=(0,i.aT)(t,n,e);let o=s.shape;e||(o=(0,r.aF)(s.shape,n));const c=(0,i.z)((0,i.g)((0,r.c)(t,"float32"),(0,r.r)(s,o)));return{mean:s,variance:(0,i.aT)(c,n,e)}}}),(0,r.o)({multiRNNCell_:function(t,a,e,n){const s=(0,r.q)(a,"data","multiRNNCell"),i=(0,r.V)(e,"c","multiRNNCell"),o=(0,r.V)(n,"h","multiRNNCell");let c=s;const l=[];for(let a=0;a<t.length;a++){const e=t[a](c,i[a],o[a]);l.push(e[0]),l.push(e[1]),c=e[1]}const u=[],m=[];for(let t=0;t<l.length;t+=2)u.push(l[t]),m.push(l[t+1]);return[u,m]}}),(0,r.o)({outerProduct_:function(t,a){const e=(0,r.q)(t,"v1","outerProduct"),n=(0,r.q)(a,"v2","outerProduct");(0,l.a)(1===e.rank&&1===n.rank,(()=>`Error in outerProduct: inputs must be rank 1, but got ranks ${e.rank} and ${n.rank}.`));const s=(0,r.r)(e,[-1,1]),o=(0,r.r)(n,[1,-1]);return(0,i.aF)(s,o)}}),(0,r.o)({pad1d_:function(t,a,e=0){return(0,l.a)(2===a.length,(()=>"Invalid number of paddings. Must be length of 2.")),(0,i.bb)(t,[a],e)}}),(0,r.o)({pad2d_:function(t,a,e=0){return(0,l.a)(2===a.length&&2===a[0].length&&2===a[1].length,(()=>"Invalid number of paddings. Must be length of 2 each.")),(0,i.bb)(t,a,e)}}),(0,r.o)({pad3d_:function(t,a,e=0){return(0,l.a)(3===a.length&&2===a[0].length&&2===a[1].length&&2===a[2].length,(()=>"Invalid number of paddings. Must be length of 2 each.")),(0,i.bb)(t,a,e)}}),(0,r.o)({pad4d_:function(t,a,e=0){return(0,l.a)(4===a.length&&2===a[0].length&&2===a[1].length&&2===a[2].length&&2===a[3].length,(()=>"Invalid number of paddings. Must be length of 2 each.")),(0,i.bb)(t,a,e)}}),(0,r.o)({pool_:function(t,a,e,n,s,o,c){null==s&&(s=[1,1]),null==o&&(o=1),0===n&&(n="valid");const u=(0,r.q)(t,"x","maxPool");let m=u,h=!1;3===u.rank&&(h=!0,m=(0,r.r)(u,[1,u.shape[0],u.shape[1],u.shape[2]])),(0,l.a)((0,r.P)(o,s),(()=>`Error in pool: Either strides or dilations must be 1. Got strides ${o} and dilations '${s}'`));const d=(0,r.c8)(m.shape,a,o,s,n),b=[d.dilationHeight,d.dilationWidth];let p;p="same"===n?function(t,a){const e=t.map(((t,e)=>t+(t-1)*(a[e]-1))).map((t=>t-1)),n=e.map((t=>Math.floor(t/2))),r=e.map(((t,a)=>t-n[a]));return e.map(((t,a)=>[n[a],r[a]]))}([d.filterHeight,d.filterWidth],b):[[0,0],[0,0]];const g=1===b[0]&&1===b[1],[f,k]=function(t,a,e){const n=e.map((t=>t[0])),r=e.map((t=>t[1])),s=t.concat(n,r),i=a.map(((t,a)=>(t-s[a]%t)%t)),o=r.map(((t,a)=>t+i[a]));return[a.map(((t,a)=>[n[a],o[a]])),a.map(((t,a)=>[0,i[a]]))]}([d.inHeight,d.inWidth],b,p),v=g?n:"valid",y=g?m:(0,i.ba)(m,b,f),N=("avg"===e?()=>(0,i.a6)(y,a,o,v,c):()=>(0,i.a5)(y,a,o,v,c))(),E=g?N:(0,i.b9)(N,b,k);return h?(0,r.r)(E,[E.shape[1],E.shape[2],E.shape[3]]):E}}),(0,r.o)({rand_:function(t,a,e){const n=(0,l.s)(t);let s=null;if(null==e||"float32"===e)s=new Float32Array(n);else if("int32"===e)s=new Int32Array(n);else{if("bool"!==e)throw new Error(`Unknown data type ${e}`);s=new Uint8Array(n)}for(let t=0;t<n;t++)s[t]=a();return r.E.makeTensor(s,t,e)}}),(0,r.o)({randomGamma_:function(t,a,e=1,n="float32",s){if(null==e&&(e=1),null==n&&(n="float32"),"float32"!==n&&"int32"!==n)throw new Error(`Unsupported data type ${n}`);const o=new i.be(a,e,n,s),c=(0,r.n)(t,n);for(let t=0;t<c.values.length;t++)c.values[t]=o.nextValue();return c.toTensor()}}),(0,r.o)({randomNormal_:function(t,a=0,e=1,n,s){if(null!=n&&"bool"===n)throw new Error(`Unsupported data type ${n}`);const o=new i.bf(a,e,n,!1,s),c=(0,r.n)(t,n);for(let t=0;t<c.values.length;t++)c.values[t]=o.nextValue();return c.toTensor()}}),(0,r.o)({reverse1d_:function(t){const a=(0,r.q)(t,"x","reverse");return(0,l.a)(1===a.rank,(()=>`Error in reverse1D: x must be rank 1 but got rank ${a.rank}.`)),(0,i.a_)(a,0)}}),(0,r.o)({reverse2d_:function(t,a){const e=(0,r.q)(t,"x","reverse");return(0,l.a)(2===e.rank,(()=>`Error in reverse2D: x must be rank 2 but got rank ${e.rank}.`)),(0,i.a_)(e,a)}}),(0,r.o)({reverse3d_:function(t,a){const e=(0,r.q)(t,"x","reverse");return(0,l.a)(3===e.rank,(()=>`Error in reverse3D: x must be rank 3 but got rank ${e.rank}.`)),(0,i.a_)(e,a)}}),(0,r.o)({reverse4d_:function(t,a){const e=(0,r.q)(t,"x","reverse");return(0,l.a)(4===e.rank,(()=>`Error in reverse4D: x must be rank 4 but got rank ${e.rank}.`)),(0,i.a_)(e,a)}}),(0,r.o)({separableConv2d_:function(t,a,e,n,s,o=[1,1],c="NHWC"){const u=(0,r.q)(t,"x","separableConv2d"),m=(0,r.q)(a,"depthwiseFilter","separableConv2d"),h=(0,r.q)(e,"pointwiseFilter","separableConv2d");let d=u,b=!1;if(3===u.rank&&(b=!0,d=(0,r.r)(u,[1,u.shape[0],u.shape[1],u.shape[2]])),"NCHW"===c)throw new Error("separableConv2d currently does not support dataFormat NCHW; only NHWC is supported");(0,l.a)(4===d.rank,(()=>`Error in separableConv2d: input must be rank 4, but got rank ${d.rank}.`)),(0,l.a)(4===m.rank,(()=>`Error in separableConv2d: depthwise filter must be rank 4, but got rank ${m.rank}.`)),(0,l.a)(4===h.rank,(()=>`Error in separableConv2d: pointwise filter must be rank 4, but got rank ${m.rank}.`)),(0,l.a)(1===h.shape[0],(()=>`Error in separableConv2d: the first dimension of pointwise filter  must be 1, but got ${h.shape[0]}.`)),(0,l.a)(1===h.shape[1],(()=>`Error in separableConv2d: the second dimension of pointwise filter must be 1, but got ${h.shape[1]}.`));const p=m.shape[2],g=m.shape[3];(0,l.a)(h.shape[2]===p*g,(()=>`Error in separableConv2d: the third dimension of pointwise filter must be ${p*g}, but got ${h.shape[2]}.`));const f=(0,i.a8)(d,m,n,s,c,o),k=(0,i.ac)(f,h,1,"valid",c);return b?(0,r.r)(k,[k.shape[1],k.shape[2],k.shape[3]]):k}}),(0,r.o)({slice1d_:function(t,a,e){const n=(0,r.q)(t,"x","slice1d");return(0,l.a)(1===n.rank,(()=>`slice1d expects a rank-1 tensor, but got a rank-${n.rank} tensor`)),(0,i.c)(n,[a],[e])}}),(0,r.o)({slice2d_:function(t,a,e){const n=(0,r.q)(t,"x","slice2d");return(0,l.a)(2===n.rank,(()=>`slice2d expects a rank-2 tensor, but got a rank-${n.rank} tensor`)),(0,i.c)(n,a,e)}}),(0,r.o)({slice3d_:function(t,a,e){const n=(0,r.q)(t,"x","slice3d");return(0,l.a)(3===n.rank,(()=>`slice3d expects a rank-3 tensor, but got a rank-${n.rank} tensor`)),(0,i.c)(n,a,e)}}),(0,r.o)({slice4d_:function(t,a,e){const n=(0,r.q)(t,"x","slice4d");return(0,l.a)(4===n.rank,(()=>`slice4d expects a rank-4 tensor, but got a rank-${n.rank} tensor`)),(0,i.c)(n,a,e)}}),(0,r.o)({unsortedSegmentSum_:function(t,a,e){const n=(0,r.q)(t,"x","unsortedSegmentSum"),s=(0,r.q)(a,"segmentIds","unsortedSegmentSum","int32");(0,l.a)((0,l.c)(e),(()=>"numSegments must be of dtype int"));const i={x:n,segmentIds:s},o={numSegments:e};return r.E.runKernel(r.cM,i,o)}}),(0,r.o)({movingAverage_:function(t,a,e,n,o=!0){const c=(0,r.q)(t,"v","movingAverage"),u=(0,r.q)(a,"x","movingAverage"),m=(0,r.q)(e,"decay","movingAverage");(0,r.cO)(c,u),(0,l.a)((0,l.b)(c.shape,u.shape),(()=>"Shape mismatch in v and x"));const h=(0,s.s)(1),d=(0,i.g)(h,m);let b=(0,r.m)((0,i.g)(u,c),d);if(o){(0,l.a)(null!=n,(()=>"When using zeroDebias: true, step is required."));const t=(0,r.q)(n,"step","movingAverage");b=(0,i.d)(b,(0,i.g)(h,(0,i.p)(m,t)))}return(0,i.b)(c,b)}}),(0,r.o)({dropout_:function(t,a,e,n){const s=(0,r.q)(t,"x","dropout");if((0,l.a)("float32"===s.dtype,(()=>`x has to be a floating point tensor since it's going to be scaled, but got a ${s.dtype} tensor instead.`)),(0,l.a)(a>=0&&a<1,(()=>`rate must be a float in the range [0, 1), but got ${a}.`)),0===a)return t instanceof r.T?s.clone():s;const o=function(t,a){if(null==a)return t.shape.slice();if((0,l.b)(t.shape,a))return a;if(t.shape.length===a.length){const e=[];for(let n=0;n<t.shape.length;n++)null==a[n]&&null!=t.shape[n]?e.push(t.shape[n]):e.push(a[n]);return e}return a}(s,e),c=1-a,u=(0,i.d)((0,i.N)((0,i.b)((0,i.ah)(o,0,1,"float32",n),c)),c);return(0,r.m)(s,u)}});class d extends class{getClassName(){return this.constructor.className}static fromConfig(t,a){return new t(a)}}{minimize(t,a=!1,e){const{value:n,grads:r}=this.computeGradients(t,e);if(null!=e){const t=e.map((t=>({name:t.name,tensor:r[t.name]})));this.applyGradients(t)}else this.applyGradients(r);return(0,s.d)(r),a?n:(n.dispose(),null)}get iterations(){return null==this.iterations_&&(this.iterations_=0),this.iterations_}incrementIterations(){this.iterations_=this.iterations+1}computeGradients(t,a){return(0,i.bg)(t,a)}dispose(){null!=this.iterations_&&(0,s.d)(this.iterations_)}async saveIterations(){return null==this.iterations_&&(this.iterations_=0),{name:"iter",tensor:(0,s.s)(this.iterations_,"int32")}}async getWeights(){throw new Error("getWeights() is not implemented for this optimizer yet.")}async setWeights(t){throw new Error(`setWeights() is not implemented for this optimizer class ${this.getClassName()}`)}async extractIterations(t){return this.iterations_=(await t[0].tensor.data())[0],t.slice(1)}}Object.defineProperty(d,Symbol.hasInstance,{value:t=>null!=t.minimize&&null!=t.computeGradients&&null!=t.applyGradients});class b extends d{constructor(t,a,e=null){super(),this.learningRate=t,this.rho=a,this.epsilon=e,this.accumulatedGrads=[],this.accumulatedUpdates=[],null==e&&(this.epsilon=r.E.backend.epsilon())}applyGradients(t){(Array.isArray(t)?t.map((t=>t.name)):Object.keys(t)).forEach(((a,e)=>{const n=r.E.registeredVariables[a];null==this.accumulatedGrads[e]&&(this.accumulatedGrads[e]={originalName:`${a}/accum_grad`,variable:(0,s.t)((()=>(0,i.ae)(n).variable(!1)))}),null==this.accumulatedUpdates[e]&&(this.accumulatedUpdates[e]={originalName:`${a}/accum_var`,variable:(0,s.t)((()=>(0,i.ae)(n).variable(!1)))});const o=Array.isArray(t)?t[e].tensor:t[a];if(null==o)return;const c=this.accumulatedGrads[e].variable,l=this.accumulatedUpdates[e].variable;(0,s.t)((()=>{const t=(0,i.b)((0,r.m)(c,this.rho),(0,r.m)((0,i.z)(o),1-this.rho)),a=(0,r.m)((0,i.d)((0,i.A)((0,i.b)(l,this.epsilon)),(0,i.A)((0,i.b)(c,this.epsilon))),o),e=(0,i.b)((0,r.m)(l,this.rho),(0,r.m)((0,i.z)(a),1-this.rho));c.assign(t),l.assign(e);const s=(0,i.b)((0,r.m)(a,-this.learningRate),n);n.assign(s)}))})),this.incrementIterations()}dispose(){null!=this.accumulatedUpdates&&((0,s.d)(this.accumulatedGrads.map((t=>t.variable))),(0,s.d)(this.accumulatedUpdates.map((t=>t.variable))))}async getWeights(){const t=[...this.accumulatedGrads,...this.accumulatedUpdates];return[await this.saveIterations()].concat(t.map((t=>({name:t.originalName,tensor:t.variable}))))}async setWeights(t){const a=(t=await this.extractIterations(t)).length/2;this.accumulatedGrads=t.slice(0,a).map((t=>({originalName:t.name,variable:t.tensor.variable(!1)}))),this.accumulatedUpdates=t.slice(a,2*a).map((t=>({originalName:t.name,variable:t.tensor.variable(!1)})))}getConfig(){return{learningRate:this.learningRate,rho:this.rho,epsilon:this.epsilon}}static fromConfig(t,a){return new t(a.learningRate,a.rho,a.epsilon)}}b.className="Adadelta",m(b);class p extends d{constructor(t,a=.1){super(),this.learningRate=t,this.initialAccumulatorValue=a,this.accumulatedGrads=[]}applyGradients(t){(Array.isArray(t)?t.map((t=>t.name)):Object.keys(t)).forEach(((a,e)=>{const n=r.E.registeredVariables[a];if(null==this.accumulatedGrads[e]){const t=!1;this.accumulatedGrads[e]={originalName:`${a}/accumulator`,variable:(0,s.t)((()=>(0,i.an)(n.shape,this.initialAccumulatorValue).variable(t)))}}const o=Array.isArray(t)?t[e].tensor:t[a];if(null==o)return;const c=this.accumulatedGrads[e].variable;(0,s.t)((()=>{const t=(0,i.b)(c,(0,i.z)(o));c.assign(t);const a=(0,i.b)((0,r.m)((0,i.d)(o,(0,i.A)((0,i.b)(t,r.E.backend.epsilon()))),-this.learningRate),n);n.assign(a)}))})),this.incrementIterations()}dispose(){null!=this.accumulatedGrads&&(0,s.d)(this.accumulatedGrads.map((t=>t.variable)))}async getWeights(){return[await this.saveIterations()].concat(this.accumulatedGrads.map((t=>({name:t.originalName,tensor:t.variable}))))}async setWeights(t){t=await this.extractIterations(t),this.accumulatedGrads=t.map((t=>({originalName:t.name,variable:t.tensor.variable(!1)})))}getConfig(){return{learningRate:this.learningRate,initialAccumulatorValue:this.initialAccumulatorValue}}static fromConfig(t,a){return new t(a.learningRate,a.initialAccumulatorValue)}}p.className="Adagrad",m(p);class g extends d{constructor(t,a,e,n=null){super(),this.learningRate=t,this.beta1=a,this.beta2=e,this.epsilon=n,this.accumulatedFirstMoment=[],this.accumulatedSecondMoment=[],(0,s.t)((()=>{this.accBeta1=(0,s.s)(a).variable(),this.accBeta2=(0,s.s)(e).variable()})),null==n&&(this.epsilon=r.E.backend.epsilon())}applyGradients(t){const a=Array.isArray(t)?t.map((t=>t.name)):Object.keys(t);(0,s.t)((()=>{const e=(0,i.g)(1,this.accBeta1),n=(0,i.g)(1,this.accBeta2);a.forEach(((a,o)=>{const c=r.E.registeredVariables[a];null==this.accumulatedFirstMoment[o]&&(this.accumulatedFirstMoment[o]={originalName:`${a}/m`,variable:(0,s.t)((()=>(0,i.ae)(c).variable(!1)))}),null==this.accumulatedSecondMoment[o]&&(this.accumulatedSecondMoment[o]={originalName:`${a}/v`,variable:(0,s.t)((()=>(0,i.ae)(c).variable(!1)))});const l=Array.isArray(t)?t[o].tensor:t[a];if(null==l)return;const u=this.accumulatedFirstMoment[o].variable,m=this.accumulatedSecondMoment[o].variable,h=(0,i.b)((0,r.m)(u,this.beta1),(0,r.m)(l,1-this.beta1)),d=(0,i.b)((0,r.m)(m,this.beta2),(0,r.m)((0,i.z)(l),1-this.beta2)),b=(0,i.d)(h,e),p=(0,i.d)(d,n);u.assign(h),m.assign(d);const g=(0,i.b)((0,r.m)((0,i.d)(b,(0,i.b)((0,i.A)(p),this.epsilon)),-this.learningRate),c);c.assign(g)})),this.accBeta1.assign((0,r.m)(this.accBeta1,this.beta1)),this.accBeta2.assign((0,r.m)(this.accBeta2,this.beta2))})),this.incrementIterations()}dispose(){this.accBeta1.dispose(),this.accBeta2.dispose(),null!=this.accumulatedFirstMoment&&(0,s.d)(this.accumulatedFirstMoment.map((t=>t.variable))),null!=this.accumulatedSecondMoment&&(0,s.d)(this.accumulatedSecondMoment.map((t=>t.variable)))}async getWeights(){const t=[...this.accumulatedFirstMoment,...this.accumulatedSecondMoment];return[await this.saveIterations()].concat(t.map((t=>({name:t.originalName,tensor:t.variable}))))}async setWeights(t){t=await this.extractIterations(t),(0,s.t)((()=>{this.accBeta1.assign((0,i.p)(this.beta1,this.iterations_+1)),this.accBeta2.assign((0,i.p)(this.beta2,this.iterations_+1))}));const a=t.length/2;this.accumulatedFirstMoment=t.slice(0,a).map((t=>({originalName:t.name,variable:t.tensor.variable(!1)}))),this.accumulatedSecondMoment=t.slice(a,2*a).map((t=>({originalName:t.name,variable:t.tensor.variable(!1)})))}getConfig(){return{learningRate:this.learningRate,beta1:this.beta1,beta2:this.beta2,epsilon:this.epsilon}}static fromConfig(t,a){return new t(a.learningRate,a.beta1,a.beta2,a.epsilon)}}g.className="Adam",m(g);class f extends d{constructor(t,a,e,n=null,i=0){super(),this.learningRate=t,this.beta1=a,this.beta2=e,this.epsilon=n,this.decay=i,this.accumulatedFirstMoment=[],this.accumulatedWeightedInfNorm=[],(0,s.t)((()=>{this.iteration=(0,s.s)(0).variable(),this.accBeta1=(0,s.s)(a).variable()})),null==n&&(this.epsilon=r.E.backend.epsilon())}applyGradients(t){const a=Array.isArray(t)?t.map((t=>t.name)):Object.keys(t);(0,s.t)((()=>{const e=(0,i.g)(1,this.accBeta1),n=(0,i.d)(-this.learningRate,(0,i.b)((0,r.m)(this.iteration,this.decay),1));a.forEach(((a,s)=>{const o=r.E.registeredVariables[a];null==this.accumulatedFirstMoment[s]&&(this.accumulatedFirstMoment[s]={originalName:`${a}/m`,variable:(0,i.ae)(o).variable(!1)}),null==this.accumulatedWeightedInfNorm[s]&&(this.accumulatedWeightedInfNorm[s]={originalName:`${a}/v`,variable:(0,i.ae)(o).variable(!1)});const c=Array.isArray(t)?t[s].tensor:t[a];if(null==c)return;const l=this.accumulatedFirstMoment[s].variable,u=this.accumulatedWeightedInfNorm[s].variable,m=(0,i.b)((0,r.m)(l,this.beta1),(0,r.m)(c,1-this.beta1)),h=(0,r.m)(u,this.beta2),d=(0,i._)(c),b=(0,i.m)(h,d);l.assign(m),u.assign(b);const p=(0,i.b)((0,r.m)((0,i.d)(n,e),(0,i.d)(m,(0,i.b)(b,this.epsilon))),o);o.assign(p)})),this.iteration.assign((0,i.b)(this.iteration,1)),this.accBeta1.assign((0,r.m)(this.accBeta1,this.beta1))})),this.incrementIterations()}dispose(){this.accBeta1.dispose(),this.iteration.dispose(),null!=this.accumulatedFirstMoment&&(0,s.d)(this.accumulatedFirstMoment.map((t=>t.variable))),null!=this.accumulatedWeightedInfNorm&&(0,s.d)(this.accumulatedWeightedInfNorm.map((t=>t.variable)))}async getWeights(){throw new Error("getWeights() is not implemented for Adamax yet.")}async setWeights(t){throw new Error("setWeights() is not implemented for Adamax yet.")}getConfig(){return{learningRate:this.learningRate,beta1:this.beta1,beta2:this.beta2,epsilon:this.epsilon,decay:this.decay}}static fromConfig(t,a){return new t(a.learningRate,a.beta1,a.beta2,a.epsilon,a.decay)}}f.className="Adamax",m(f);class k extends d{constructor(t){super(),this.learningRate=t,this.setLearningRate(t)}applyGradients(t){(Array.isArray(t)?t.map((t=>t.name)):Object.keys(t)).forEach(((a,e)=>{const n=Array.isArray(t)?t[e].tensor:t[a];if(null==n)return;const o=r.E.registeredVariables[a];(0,s.t)((()=>{const t=(0,i.b)((0,r.m)(this.c,n),o);o.assign(t)}))})),this.incrementIterations()}setLearningRate(t){this.learningRate=t,null!=this.c&&this.c.dispose(),this.c=(0,s.k)((0,s.s)(-t))}dispose(){this.c.dispose()}async getWeights(){return[await this.saveIterations()]}async setWeights(t){if(0!==(t=await this.extractIterations(t)).length)throw new Error("SGD optimizer does not have settable weights.")}getConfig(){return{learningRate:this.learningRate}}static fromConfig(t,a){return new t(a.learningRate)}}k.className="SGD",m(k);class v extends k{constructor(t,a,e=!1){super(t),this.learningRate=t,this.momentum=a,this.useNesterov=e,this.accumulations=[],this.m=(0,s.s)(this.momentum)}applyGradients(t){(Array.isArray(t)?t.map((t=>t.name)):Object.keys(t)).forEach(((a,e)=>{const n=r.E.registeredVariables[a];if(null==this.accumulations[e]){const t=!1;this.accumulations[e]={originalName:`${a}/momentum`,variable:(0,s.t)((()=>(0,i.ae)(n).variable(t)))}}const o=this.accumulations[e].variable,c=Array.isArray(t)?t[e].tensor:t[a];null!=c&&(0,s.t)((()=>{let t;const a=(0,i.b)((0,r.m)(this.m,o),c);t=this.useNesterov?(0,i.b)((0,r.m)(this.c,(0,i.b)(c,(0,r.m)(a,this.m))),n):(0,i.b)((0,r.m)(this.c,a),n),o.assign(a),n.assign(t)}))})),this.incrementIterations()}dispose(){this.m.dispose(),null!=this.accumulations&&(0,s.d)(this.accumulations.map((t=>t.variable)))}setMomentum(t){this.momentum=t}async getWeights(){return[await this.saveIterations()].concat(this.accumulations.map((t=>({name:t.originalName,tensor:t.variable}))))}async setWeights(t){t=await this.extractIterations(t),this.accumulations=t.map((t=>({originalName:t.name,variable:t.tensor.variable(!1)})))}getConfig(){return{learningRate:this.learningRate,momentum:this.momentum,useNesterov:this.useNesterov}}static fromConfig(t,a){return new t(a.learningRate,a.momentum,a.useNesterov)}}v.className="Momentum",m(v);class y extends d{constructor(t,a=.9,e=0,n=null,s=!1){if(super(),this.learningRate=t,this.decay=a,this.momentum=e,this.epsilon=n,this.accumulatedMeanSquares=[],this.accumulatedMoments=[],this.accumulatedMeanGrads=[],this.centered=s,null==n&&(this.epsilon=r.E.backend.epsilon()),null==t)throw new Error("learningRate for RMSPropOptimizer must be defined.")}applyGradients(t){(Array.isArray(t)?t.map((t=>t.name)):Object.keys(t)).forEach(((a,e)=>{const n=r.E.registeredVariables[a],o=!1;null==this.accumulatedMeanSquares[e]&&(this.accumulatedMeanSquares[e]={originalName:`${a}/rms`,variable:(0,s.t)((()=>(0,i.ae)(n).variable(o)))}),null==this.accumulatedMoments[e]&&(this.accumulatedMoments[e]={originalName:`${a}/momentum`,variable:(0,s.t)((()=>(0,i.ae)(n).variable(o)))}),null==this.accumulatedMeanGrads[e]&&this.centered&&(this.accumulatedMeanGrads[e]={originalName:`${a}/mg`,variable:(0,s.t)((()=>(0,i.ae)(n).variable(o)))});const c=Array.isArray(t)?t[e].tensor:t[a];if(null==c)return;const l=this.accumulatedMeanSquares[e].variable,u=this.accumulatedMoments[e].variable;(0,s.t)((()=>{const t=(0,i.b)((0,r.m)(l,this.decay),(0,r.m)((0,i.z)(c),1-this.decay));if(this.centered){const a=this.accumulatedMeanGrads[e].variable,s=(0,i.b)((0,r.m)(a,this.decay),(0,r.m)(c,1-this.decay)),o=(0,i.d)((0,r.m)(c,this.learningRate),(0,i.A)((0,i.g)(t,(0,i.b)((0,i.z)(s),this.epsilon)))),m=(0,i.b)((0,r.m)(u,this.momentum),o);l.assign(t),a.assign(s),u.assign(m);const h=(0,i.g)(n,m);n.assign(h)}else{const t=(0,i.b)((0,r.m)(l,this.decay),(0,r.m)((0,i.z)(c),1-this.decay)),a=(0,i.b)((0,r.m)(u,this.momentum),(0,i.d)((0,r.m)(c,this.learningRate),(0,i.A)((0,i.b)(t,this.epsilon))));l.assign(t),u.assign(a);const e=(0,i.g)(n,a);n.assign(e)}}))})),this.incrementIterations()}dispose(){null!=this.accumulatedMeanSquares&&(0,s.d)(this.accumulatedMeanSquares.map((t=>t.variable))),null!=this.accumulatedMeanGrads&&this.centered&&(0,s.d)(this.accumulatedMeanGrads.map((t=>t.variable))),null!=this.accumulatedMoments&&(0,s.d)(this.accumulatedMoments.map((t=>t.variable)))}async getWeights(){const t=[...this.accumulatedMeanSquares,...this.accumulatedMoments];return this.centered&&t.push(...this.accumulatedMeanGrads),[await this.saveIterations()].concat(t.map((t=>({name:t.originalName,tensor:t.variable}))))}async setWeights(t){t=await this.extractIterations(t);const a=this.centered?t.length/3:t.length/2,e=!1;this.accumulatedMeanSquares=t.slice(0,a).map((t=>({originalName:t.name,variable:t.tensor.variable(e)}))),this.accumulatedMoments=t.slice(a,2*a).map((t=>({originalName:t.name,variable:t.tensor.variable(e)}))),this.centered&&(this.accumulatedMeanGrads=t.slice(2*a,3*a).map((t=>({originalName:t.name,variable:t.tensor.variable(e)}))))}getConfig(){return{learningRate:this.learningRate,decay:this.decay,momentum:this.momentum,epsilon:this.epsilon,centered:this.centered}}static fromConfig(t,a){return new t(a.learningRate,a.decay,a.momentum,a.epsilon,a.centered)}}y.className="RMSProp",m(y),"undefined"!=typeof requestAnimationFrame?requestAnimationFrame:"undefined"!=typeof setImmediate&&setImmediate,s.n,s.b,s.a,s.w}}]);